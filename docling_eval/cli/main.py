import json
import logging
import os
from enum import Enum
from pathlib import Path
from typing import Annotated

import typer
from tabulate import tabulate  # type: ignore

from docling_eval.benchmarks.constants import BenchMarkNames, EvaluationModality
from docling_eval.benchmarks.dpbench.create import (
    create_dpbench_e2e_dataset,
    create_dpbench_tableformer_dataset,
)
from docling_eval.benchmarks.omnidocbench.create import (
    create_omnidocbench_e2e_dataset,
    create_omnidocbench_tableformer_dataset,
)
from docling_eval.benchmarks.tableformer_huggingface_otsl.create import (
    create_fintabnet_tableformer_dataset,
)
from docling_eval.evaluators.layout_evaluator import (
    DatasetLayoutEvaluation,
    LayoutEvaluator,
)
from docling_eval.evaluators.markdown_text_evaluator import (
    DatasetMarkdownEvaluation,
    MarkdownTextEvaluator,
)
from docling_eval.evaluators.readingorder_evaluator import (
    DatasetReadingOrderEvaluation,
    ReadingOrderEvaluator,
    ReadingOrderVisualizer,
)
from docling_eval.evaluators.stats import DatasetStatistics
from docling_eval.evaluators.table_evaluator import (
    DatasetTableEvaluation,
    TableEvaluator,
)

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
log = logging.getLogger(__name__)

app = typer.Typer(
    name="docling-eval",
    no_args_is_help=True,
    add_completion=False,
    pretty_exceptions_enable=False,
)


class EvaluationTask(str, Enum):
    CREATE = "create"
    EVALUATE = "evaluate"
    VISUALIZE = "visualize"


def log_and_save_stats(
    odir: Path,
    benchmark: BenchMarkNames,
    modality: EvaluationModality,
    metric: str,
    stats: DatasetStatistics,
) -> tuple[Path, Path]:
    r"""
    For the given DatasetStatistics related to the provided benchmark/modality/metric:
    - Generate a textual table. Log it and save it in a file.
    - Generate a plot and save it in a file.

    The filenames of the generated files are derived by the benchmark/modality/metric

    Returns
    -------
    log_filename: Path of the saved log file
    fig_filename: Path of the saved png file
    """
    log_filename = odir / f"evaluation_{benchmark.value}_{modality.value}_{metric}.txt"
    fig_filename = odir / f"evaluation_{benchmark.value}_{modality.value}_{metric}.png"

    data, headers = stats.to_table(metric)
    content = f"{benchmark.value} {modality.value} {metric}:\n\n"
    content += "mean={:.2f} median={:.2f} std={:.2f}\n\n".format(
        stats.mean, stats.median, stats.std
    )
    content += tabulate(data, headers=headers, tablefmt="github")

    log.info(content)
    with open(log_filename, "w") as fd:
        fd.write(content)
    stats.save_histogram(figname=fig_filename, name=metric)

    return log_filename, fig_filename


def create(
    modality: EvaluationModality,
    benchmark: BenchMarkNames,
    idir: Path,
    odir: Path,
    image_scale: float = 1.0,
):
    r""""""
    if not os.path.exists(idir):
        log.error(f"Benchmark directory not found: {idir}")
        return

    if odir is None:
        odir = Path("./benchmarks") / benchmark.value / modality.value

    if benchmark == BenchMarkNames.DPBENCH:
        if (
            modality == EvaluationModality.END2END
            or modality == EvaluationModality.LAYOUT
        ):
            create_dpbench_e2e_dataset(
                dpbench_dir=idir, output_dir=odir, image_scale=image_scale
            )

        elif modality == EvaluationModality.TABLEFORMER:
            create_dpbench_tableformer_dataset(
                dpbench_dir=idir, output_dir=odir, image_scale=image_scale
            )

        else:
            log.error(f"{modality} is not yet implemented for {benchmark}")

    elif benchmark == BenchMarkNames.OMNIDOCBENCH:
        if (
            modality == EvaluationModality.END2END
            or modality == EvaluationModality.LAYOUT
        ):
            create_omnidocbench_e2e_dataset(
                omnidocbench_dir=idir, output_dir=odir, image_scale=image_scale
            )
        elif modality == EvaluationModality.TABLEFORMER:
            create_omnidocbench_tableformer_dataset(
                omnidocbench_dir=idir, output_dir=odir, image_scale=image_scale
            )
        else:
            log.error(f"{modality} is not yet implemented for {benchmark}")

    else:
        log.error(f"{benchmark} is not yet implemented")


def evaluate(
    modality: EvaluationModality, benchmark: BenchMarkNames, idir: Path, odir: Path
):
    r""""""
    if not os.path.exists(idir):
        log.error(f"Benchmark directory not found: {idir}")

    # Save the evaluation
    save_fn = odir / f"evaluation_{benchmark.value}_{modality.value}.json"

    if modality == EvaluationModality.END2END:
        log.error("not supported")

    elif modality == EvaluationModality.LAYOUT:
        layout_evaluator = LayoutEvaluator()
        layout_evaluation = layout_evaluator(idir, split="test")

        with open(save_fn, "w") as fd:
            json.dump(layout_evaluation.model_dump(), fd, indent=2, sort_keys=True)

    elif modality == EvaluationModality.TABLEFORMER:
        table_evaluator = TableEvaluator()
        table_evaluation = table_evaluator(idir, split="test")

        with open(save_fn, "w") as fd:
            json.dump(table_evaluation.model_dump(), fd, indent=2, sort_keys=True)

    elif modality == EvaluationModality.READING_ORDER:
        readingorder_evaluator = ReadingOrderEvaluator()
        readingorder_evaluation = readingorder_evaluator(idir, split="test")

        with open(save_fn, "w") as fd:
            json.dump(
                readingorder_evaluation.model_dump(),
                fd,
                indent=2,
                sort_keys=True,
                ensure_ascii=False,
            )

    elif modality == EvaluationModality.MARKDOWN_TEXT:
        md_evaluator = MarkdownTextEvaluator()
        md_evaluation = md_evaluator(idir, split="test")

        with open(save_fn, "w") as fd:
            json.dump(
                md_evaluation.model_dump(),
                fd,
                indent=2,
                sort_keys=True,
                ensure_ascii=False,
            )

    elif modality == EvaluationModality.CODEFORMER:
        pass

    log.info("The evaluation has been saved in '%s'", save_fn)


def visualise(
    modality: EvaluationModality, benchmark: BenchMarkNames, idir: Path, odir: Path
):

    metrics_filename = odir / f"evaluation_{benchmark.value}_{modality.value}.json"

    if modality == EvaluationModality.END2END:
        pass

    elif modality == EvaluationModality.LAYOUT:
        with open(metrics_filename, "r") as fd:
            layout_evaluation = DatasetLayoutEvaluation.parse_file(metrics_filename)

        # Save layout statistics for mAP
        log_filename, _ = log_and_save_stats(
            odir, benchmark, modality, "mAP[0.5_0.95]", layout_evaluation.mAP_stats
        )

        # Append to layout statistics the mAP classes
        data, headers = layout_evaluation.to_table()
        content = "\n\n\nClass mAP[0.5:0.95] table:\n\n"
        content += tabulate(data, headers=headers, tablefmt="github")
        log.info(content)
        with open(log_filename, "a") as fd:
            fd.write(content)

    elif modality == EvaluationModality.TABLEFORMER:
        with open(metrics_filename, "r") as fd:
            table_evaluation = DatasetTableEvaluation.parse_file(metrics_filename)

        figname = (
            odir / f"evaluation_{benchmark.value}_{modality.value}-delta_row_col.png"
        )
        table_evaluation.save_histogram_delta_row_col(figname=figname)

        # TEDS struct-with-text
        log_and_save_stats(
            odir, benchmark, modality, "TEDS_struct-with-text", table_evaluation.TEDS
        )

        # TEDS struct-only
        log_and_save_stats(
            odir, benchmark, modality, "TEDS_struct-only", table_evaluation.TEDS_struct
        )

    elif modality == EvaluationModality.READING_ORDER:
        with open(metrics_filename, "r") as fd:
            ro_evaluation = DatasetReadingOrderEvaluation.parse_file(metrics_filename)
        # ARD
        log_and_save_stats(
            odir, benchmark, modality, "ARD_norm", ro_evaluation.ard_stats
        )

        # Weighted ARD
        log_and_save_stats(
            odir, benchmark, modality, "weighted_ARD", ro_evaluation.w_ard_stats
        )

        # Generate visualizations of the reading order across the GT and the prediction
        ro_visualizer = ReadingOrderVisualizer()
        ro_visualizer(idir, metrics_filename, odir, split="test")

    elif modality == EvaluationModality.MARKDOWN_TEXT:
        with open(metrics_filename, "r") as fd:
            md_evaluation = DatasetMarkdownEvaluation.parse_file(metrics_filename)
        log_and_save_stats(odir, benchmark, modality, "BLEU", md_evaluation.bleu_stats)

    elif modality == EvaluationModality.CODEFORMER:
        pass


@app.command(no_args_is_help=True)
def main(
    task: Annotated[
        EvaluationTask,
        typer.Option(
            ...,  # EvaluationTask.CREATE,
            "-t",  # Short name
            "--task",  # Long name
            help="Evaluation task",
        ),
    ],
    modality: Annotated[
        EvaluationModality,
        typer.Option(
            ...,  # EvaluationModality.TABLEFORMER,
            "-m",  # Short name
            "--modality",  # Long name
            help="Evaluation modality",
        ),
    ],
    benchmark: Annotated[
        BenchMarkNames,
        typer.Option(
            ...,  # BenchMarkNames.DPBENCH,
            "-b",  # Short name
            "--benchmark",  # Long name
            help="Benchmark name",
        ),
    ],
    idir: Annotated[
        Path,
        typer.Option(
            ...,
            "-i",  # Short name
            "--input-dir",  # Long name
            help="Input directory",
        ),
    ],
    odir: Annotated[
        Path,
        typer.Option(
            ...,
            "-o",  # Short name
            "--output-dir",  # Long name
            help="Output directory",
        ),
    ],
):
    # Dispatch the command
    if task == EvaluationTask.CREATE:
        create(modality, benchmark, idir, odir)

    elif task == EvaluationTask.EVALUATE:
        evaluate(modality, benchmark, idir, odir)

    elif task == EvaluationTask.VISUALIZE:
        visualise(modality, benchmark, idir, odir)

    else:
        log.error("Unsupported command: '%s'", task.value)


if __name__ == "__main__":
    app()
